{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"},{"sourceId":13904981,"sourceType":"datasetVersion","datasetId":8762382}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"5d7ae83b","cell_type":"markdown","source":"# Predicting Student Exam Scores  \n### Kaggle Playground Series S6E1\n\nThe goal is to predict students' exam scores using tabular data. \n\nModels used:\n- XGBoost (tree-based gradient boosting)\n- TabM (deep learning model for tabular data)","metadata":{}},{"id":"74bfa593","cell_type":"code","source":"# Load training and test datasets\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import List\n\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s6e1/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s6e1/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T05:51:10.378390Z","iopub.execute_input":"2026-02-03T05:51:10.378903Z","iopub.status.idle":"2026-02-03T05:51:12.592005Z","shell.execute_reply.started":"2026-02-03T05:51:10.378857Z","shell.execute_reply":"2026-02-03T05:51:12.590752Z"}},"outputs":[],"execution_count":1},{"id":"92815ac9","cell_type":"code","source":"# Load training and test datasets\norig = pd.read_csv(\"/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv\")\nprint(\"Original data shape:\", orig.shape )\norig.head(1)","metadata":{},"outputs":[],"execution_count":null},{"id":"4c53f180","cell_type":"code","source":"print(\"Train Shape: \", train.shape)\ntrain.head()","metadata":{},"outputs":[],"execution_count":null},{"id":"b602a0f3","cell_type":"code","source":"def plot_features_dual_axis(\n    df: pd.DataFrame, cols: List[str], target_col: str, n_bins: int=10, n_wide: int=3, figsize_per_plot: tuple[float,float] = (5,4), int_as_cat_unique_max: int | None = 20, cat_order: dict[str,list] | None = None,\n):\n    BAR_COLOR = \"tab:blue\"\n    LINE_COLOR = \"tab:orange\"\n\n    def is_categorical(s: pd.Series) -> bool:\n        return s.dtype == \"object\" or pd.api.types.is_string_dtype(s)\n\n    n_cols = len(cols)\n    n_rows = int(np.ceil(n_cols / n_wide))\n\n    fig, axes = plt.subplots(n_rows, n_wide, figsize=(figsize_per_plot[0] * n_wide, figsize_per_plot[1] * n_rows),)\n\n    axes = np.atleast_1d(axes).ravel()\n\n    for i, col in enumerate(cols):\n        ax1 = axes[i]\n        col_safe = col.replace(\"_\", r\"\\_\")\n\n        n_nan = df[col].isna().sum()\n        n_unique = df[col].nunique(dropna=True)\n\n        tmp = df[[col, target_col]].dropna()\n        if tmp.empty:\n            ax1.set_title(\n                rf\"$\\bf{{{col_safe}}}$\"\n                f\"\\n(empty after dropna; {n_unique} unique, {n_nan} nan)\"\n            )\n            continue\n\n        x = tmp[col]\n        y = tmp[target_col]\n\n        # True Categorical\n        if is_categorical(x):\n            type_str = \"categorical\"\n\n            counts = x.value_counts()\n            mean_y = tmp.groupby(col)[target_col].mean()\n\n            if cat_order is not None and col in cat_order:\n                desired = list(cat_order[col])\n                ordered = [c for c in desired if c in counts.index]\n                remaining = [c for c in counts.index if c not in ordered]\n                final_order = ordered + remaining\n            else:\n                final_order = sorted(counts.index)\n\n            counts = counts.loc[final_order]\n            mean_y = mean_y.loc[final_order]\n\n            xpos = np.arange(len(final_order))\n\n            ax1.bar(xpos, counts.values, alpha=0.6, color=BAR_COLOR)\n            ax1.set_xlabel(col)\n            ax1.set_ylabel(\"Count\", color = BAR_COLOR)\n            ax1.tick_params(axis=\"y\", colors = BAR_COLOR)\n            ax1.set_xticks(xpos)\n            ax1.set_xticklabels(final_order, rotation=45, ha='right')\n\n            ax2 = ax1.twinx()\n            ax2.plot(xpos, mean_y.values, marker=\"o\", color=LINE_COLOR)\n            ax2.set_ylabel(f\"Mean {target_col}\", color = LINE_COLOR)\n            ax2.tick_params(axis=\"y\", colors=LINE_COLOR)\n\n            ax1.set_title(\n                rf\"$\\bf{{{col_safe}}}$: Count vs Mean {target_col}\"\n                f\"\\n({type_str} with {n_unique} unique and {n_nan} nan)\"\n            )\n\n        else:\n            type_str = \"numeric\"\n\n            xvals = x.values\n            yvals = y.values\n\n            mask = np.isfinite(xvals) &  np.isfinite(yvals)\n\n            xvals = xvals[mask]\n            yvals = yvals[mask]\n\n            if len(xvals) == 0:\n                ax1.set_title(\n                    rf\"$\\bf{{{col_safe}}}$\"\n                    f\"\\n({type_str} with {n_unique} unique and {n_nan} nan)\"\n                )\n                continue\n\n            unique_vals = np.sort(np.unique(xvals))\n            n_unique_eff = len(unique_vals)\n\n            # --- Case 1: int-as-categorical ---\n            if int_as_cat_unique_max is not None and n_unique_eff <= int_as_cat_unique_max:\n                counts = np.array([(xvals == v).sum() for v in unique_vals])\n                mean_y = np.array([yvals[xvals == v].mean() for v in unique_vals])\n\n                xpos = np.arange(n_unique_eff)\n\n                ax1.bar(xpos, counts, alpha=0.6, color=BAR_COLOR)\n                ax1.set_xlabel(col)\n                ax1.set_ylabel(\"Count\", color=BAR_COLOR)\n                ax1.tick_params(axis=\"y\", colors=BAR_COLOR)\n\n                rotate = 45 if n_unique_eff > n_bins else 0\n                step = max(int(np.ceil(n_unique_eff / n_bins)), 1)\n                tick_idx = np.arange(0, n_unique_eff, step)\n\n                if pd.api.types.is_integer_dtype(x):\n                    tick_labels = unique_vals[tick_idx].astype(int)\n                else:\n                    tick_labels = unique_vals[tick_idx]\n\n                ax1.set_xticks(tick_idx)\n                ax1.set_xticklabels(\n                    tick_labels,\n                    rotation=rotate,\n                    ha=\"right\" if rotate else \"center\",\n                )\n\n                ax2 = ax1.twinx()\n                ax2.plot(xpos, mean_y, marker=\"o\", color=LINE_COLOR)\n                ax2.set_ylabel(f\"Mean {target_col}\", color=LINE_COLOR)\n                ax2.tick_params(axis=\"y\", colors=LINE_COLOR)\n\n                ax1.set_title(\n                    rf\"$\\bf{{{col_safe}}}$: Per-Value Count vs Mean {target_col}\"\n                    f\"\\n({type_str} with {n_unique} unique and {n_nan} nan)\"\n                )\n\n            # --- Case 2: low-cardinality numeric bins ---\n            elif n_unique_eff < n_bins:\n                counts = np.array([(xvals == v).sum() for v in unique_vals])\n                mean_y = np.array([yvals[xvals == v].mean() for v in unique_vals])\n\n                width = 0.8 * (np.min(np.diff(unique_vals)) if n_unique_eff > 1 else 1.0)\n\n                ax1.bar(unique_vals, counts, width=width, alpha=0.6, color=BAR_COLOR)\n                ax1.set_xlabel(col)\n                ax1.set_ylabel(\"Count\", color=BAR_COLOR)\n                ax1.tick_params(axis=\"y\", colors=BAR_COLOR)\n\n                ax2 = ax1.twinx()\n                ax2.plot(unique_vals, mean_y, marker=\"o\", color=LINE_COLOR)\n                ax2.set_ylabel(f\"Mean {target_col}\", color=LINE_COLOR)\n                ax2.tick_params(axis=\"y\", colors=LINE_COLOR)\n\n                ax1.set_title(\n                    rf\"$\\bf{{{col_safe}}}$: Per-Value Count vs Mean {target_col}\"\n                    f\"\\n({type_str} with {n_unique} unique and {n_nan} nan)\"\n                )\n\n            # --- Case 3: regular histogram ---\n            else:\n                bins = np.linspace(xvals.min(), xvals.max(), n_bins + 1)\n                bin_centers = 0.5 * (bins[:-1] + bins[1:])\n\n                counts, _ = np.histogram(xvals, bins=bins)\n                bin_idx = np.digitize(xvals, bins) - 1\n\n                mean_y = np.array([\n                    yvals[bin_idx == j].mean() if np.any(bin_idx == j) else np.nan\n                    for j in range(n_bins)\n                ])\n\n                ax1.bar(\n                    bin_centers,\n                    counts,\n                    width=(bins[1] - bins[0]),\n                    alpha=0.6,\n                    color=BAR_COLOR,\n                )\n                ax1.set_xlabel(col)\n                ax1.set_ylabel(\"Count\", color=BAR_COLOR)\n                ax1.tick_params(axis=\"y\", colors=BAR_COLOR)\n\n                ax2 = ax1.twinx()\n                ax2.plot(bin_centers, mean_y, marker=\"o\", color=LINE_COLOR)\n                ax2.set_ylabel(f\"Mean {target_col}\", color=LINE_COLOR)\n                ax2.tick_params(axis=\"y\", colors=LINE_COLOR)\n\n                ax1.set_title(\n                    rf\"$\\bf{{{col_safe}}}$: Histogram vs Mean {target_col}\"\n                    f\"\\n({type_str} with {n_unique} unique and {n_nan} nan)\"\n                )\n\n    for j in range(i + 1, len(axes)):\n        axes[j].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"454a6835","cell_type":"code","source":"FEATURES = list(train.columns[1:-1])\nprint(f\"There are {len(FEATURES)} features\")\nprint(FEATURES)","metadata":{},"outputs":[],"execution_count":null},{"id":"cfb4a827","cell_type":"code","source":"ordinal_order = {\"sleep_quality\":[\"poor\",\"average\",\"good\"],\n    \"facility_rating\":[\"low\",\"medium\",\"high\"],\n    \"exam_difficulty\":[\"easy\",\"moderate\",\"hard\"],}\n\nplot_features_dual_axis(\n    train,\n    cols=FEATURES,\n    target_col=\"exam_score\",\n    n_bins=10,\n    n_wide=3,\n    int_as_cat_unique_max=20,\n    cat_order = ordinal_order,\n)","metadata":{},"outputs":[],"execution_count":null},{"id":"9faf1fd3","cell_type":"code","source":"# !pip install pytabkit","metadata":{},"outputs":[],"execution_count":null},{"id":"1483e3fa","cell_type":"markdown","source":"# XGBoost with Pseudo Labels","metadata":{}},{"id":"52c30040","cell_type":"code","source":"# Set up cross-validation strategy\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nimport torch\nfrom pytabkit import TabM_D_Regressor","metadata":{},"outputs":[],"execution_count":null},{"id":"515f8b05","cell_type":"code","source":"# Train XGBoost model\nimport xgboost as xgb\nprint(f\"XGBoost version: {xgb.__version__}\")\n\nimport importlib.metadata\nprint(f\"TabM version: {importlib.metadata.version('pytabkit')}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"d973b50e","cell_type":"code","source":"train.head(1)","metadata":{},"outputs":[],"execution_count":null},{"id":"aa7e1967","cell_type":"code","source":"TARGET = \"exam_score\"\nCATS = [\"gender\",\"course\",\"internet_access\",\"study_method\",\"sleep_quality\",\"facility_rating\",\"exam_difficulty\"]\nFEATURES = ['age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\nprint(f\"There are {len(FEATURES)} features.\")","metadata":{},"outputs":[],"execution_count":null},{"id":"5671d22c","cell_type":"code","source":"def formula(df):\n    f = (6*df.study_hours + 0.35*df.class_attendance + 1.5*df.sleep_hours +\n                 5*(df.sleep_quality=='good') + -5*(df.sleep_quality=='poor') +\n                 10*(df.study_method=='coaching') + 5*(df.study_method=='mixed') + 2*(df.study_method=='group study') + 1*(df.study_method=='online videos') +\n                 4*(df.facility_rating=='high') + -4*(df.facility_rating=='low') )\n    return f\n\nfor df in [train, test, orig]:\n    df['formula'] = formula(df)","metadata":{},"outputs":[],"execution_count":null},{"id":"7a4f17ee","cell_type":"code","source":"ordinal_maps = {\n    \"gender\" : {\"male\":0, \"female\":1, \"other\":2},\n    \"internet_access\" : {\"no\":0, \"yes\":1},\n    \"sleep_quality\" : {\"poor\":0, \"average\":1, \"good\":2},\n    \"facility_rating\" : {\"low\":0, \"medium\":1, \"high\":2},\n    \"exam_difficulty\" : {\"easy\":0, \"moderate\":1, \"hard\":2},\n    \"course\" : {\"ba\":0, \"b.sc\":1, \"diploma\":2, \"b.tech\":3, \"b.com\":4, \"bca\":5, \"bba\":6},\n    \"study_method\" : {\"self-study\":0, \"online videos\":1, \"group study\":2, \"mixed\":3, \"coaching\":4},\n}\n\nfor df in [train, test, orig]:\n    for c in CATS:\n        df[c] = df[c].map(ordinal_maps[c]).fillna(-1).astype('int32')","metadata":{},"outputs":[],"execution_count":null},{"id":"dddc21d1","cell_type":"code","source":"NEW_CATS = []\nfor c in FEATURES:\n    n = f\"CAT_{c}\"\n    NEW_CATS.append(n)\n\n    train[n] = train[c].copy()\n    test[n] = test[c].copy()\n    orig[n] = orig[c].copy()\n\n    combine = pd.concat([train[n], test[n], orig[n]], axis=0)\n    v, _ = combine.factorize()\n    train[n] = v[:len(train)]\n    test[n] = v[len(train):len(train)+len(test)]\n    orig[n] = v[len(train)+len(test):]\n    for df in [train, test, orig]:\n        df[n] = df[n].astype(\"int32\")\n\nFEATURES += ['formula'] + NEW_CATS\nprint(f\"Now there are {len(FEATURES)} features.\")","metadata":{},"outputs":[],"execution_count":null},{"id":"c1d616f2","cell_type":"code","source":"# Train XGBoost model\nxgb_params = {\n    \"n_estimators\": 10_000,\n    \"learning_rate\": 0.01,\n    \"max_depth\": 8,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.35,\n    \"min_child_weight\": 5,\n    \"early_stopping_rounds\": 100,\n    \"eval_metric\": \"rmse\",\n    \"enable_categorical\": True,\n    \"device\": \"cuda\",\n    \"random_state\": 42\n}\n\ntabm_params = {\n    \"verbosity\": 0,\n    \"arch_type\": \"tabm-mini-normal\",\n    \"tabm_k\": 24, \n    \"num_emb_type\": \"pwl\", \n    \"d_embedding\": 8,\n    \"batch_size\": 256,\n    \"lr\": 1e-3, \n    \"n_epochs\": 100, \n    \"dropout\": 0.11, \n    \"d_block\": 256, \n    \"n_blocks\": 5, \n    \"patience\": 4, \n    \"weight_decay\": 1e-2, \n    \"device\": \"cuda\",\n    \"random_state\": 42,\n}","metadata":{},"outputs":[],"execution_count":null},{"id":"ad24c948","cell_type":"code","source":"# Set up cross-validation strategy\nN_SPLITS = 10\n\nX = train[FEATURES]\nX_test = test[FEATURES]\nX_orig = orig[FEATURES]\n\ny = train[TARGET].values\ny_orig = orig[TARGET].values\n\noof_preds_xgb = np.zeros(len(train))\ntest_preds_xgb = np.zeros(len(test))\n\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state = 42)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n    print(\"#\" * 25)\n    print(f\"### Fold {fold}\")\n    print(\"#\" * 25)\n\n    X_train, X_val = X.iloc[train_idx], X.iloc[ val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    X_train = pd.concat([X_train,X_orig],axis=0)\n    y_train = np.concatenate([y_train,y_orig],axis=0)\\\n\n    model = TabM_D_Regressor(**tabm_params)\n    model.fit(X_train, y_train, X_val, y_val, cat_col_names = NEW_CATS,)\n    \n    oof_preds = model.predict(X_val)\n    test_preds = model.predict(X_test)\n\n    X_train_xgb = pd.concat([X_train,X_val,X_test],axis=0)\n    y_train_xgb = np.concatenate([y_train,oof_preds,test_preds],axis=0)\n\n    for c in NEW_CATS:\n        X_train_xgb[c] = X_train_xgb[c].astype('category')\n    X_val_xgb = X_train_xgb.iloc[len(X_train):len(X_train)+len(X_val)]\n    X_test_xgb = X_train_xgb.iloc[len(X_train)+len(X_val):]\n\n    # TRAIN XGB WITH PSEUDO LABELS\n    model = XGBRegressor(**xgb_params)\n    model.fit(\n        X_train_xgb,\n        y_train_xgb,\n        eval_set=[(X_val_xgb, y_val)],\n        verbose=200,\n    )\n    oof_preds = model.predict(X_val_xgb)\n    oof_preds_xgb[val_idx] = oof_preds\n\n    rmse_xgb = mean_squared_error(y_val, oof_preds)**0.5\n    print(f\"Fold {fold} RMSE XGB w/ Pseudo: {rmse_xgb:.5f}\")\n\n    test_preds = model.predict(X_test_xgb)\n    test_preds_xgb += test_preds/N_SPLITS\n\n    # CLEAR MEMORY\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# OVERALL CV SCORE\nprint(\"-\" * 40)\nrmse_xgb = mean_squared_error(y, oof_preds_xgb)**0.5\nprint(f\"OOF RMSE XGB w/ Pseudo:       {rmse_xgb:.5f}\")\nprint()","metadata":{},"outputs":[],"execution_count":null},{"id":"d29e26dc","cell_type":"code","source":"# Train XGBoost model\nfig, ax = plt.subplots(figsize=(8,8))\nplot_importance(\n    model, ax=ax, importance_type=\"gain\", max_num_features=40, height=0.5\n)\nax.set_title(\"XGB Feature Importance\")\nfig.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"059126d3","cell_type":"code","source":"# Train XGBoost model\nrmse_xgb = mean_squared_error(y, oof_preds_xgb)**0.5\nprint(f\"OOF RMSE XGB w/ Pseudo:       {rmse_xgb:.5f}\")\n","metadata":{},"outputs":[],"execution_count":null},{"id":"fda5c895","cell_type":"code","source":"# Train XGBoost model\nnp.save(\"oof_xgb\",oof_preds_xgb)\nnp.save(\"test_preds_xgb\",test_preds_xgb)\n","metadata":{},"outputs":[],"execution_count":null},{"id":"6d319962","cell_type":"markdown","source":"# Create Submission CSV","metadata":{}},{"id":"6db5f93e","cell_type":"code","source":"# Load training and test datasets\nsub = pd.read_csv(\"/kaggle/input/playground-series-s6e1/sample_submission.csv\")\nsub['exam_score'] = np.clip(test_preds_xgb,19.6,100)\nsub.to_csv(\"submission.csv\",index=False)\nsub.head()\n","metadata":{},"outputs":[],"execution_count":null},{"id":"3c5a8fc6","cell_type":"code","source":"# Train XGBoost model\nplt.hist(sub['exam_score'],bins=100)\nplt.title(\"XGB w/ Pseudo Test Preds\")\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"28bd518d","cell_type":"markdown","source":"\n## Final Notes\n\n- XGBoost is used as a strong tree-based baseline for tabular data.\n- TabM is included to capture smoother non-linear feature interactions.\n- These models often complement each other well in ensembles.\n\nThis notebook is structured to be understandable for:\n- Kaggle participants\n- Interviewers\n- Anyone learning applied machine learning on tabular data\n","metadata":{}}]}